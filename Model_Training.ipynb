{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"Model_Training.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"aac51ce6"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from spacy.tokens import DocBin\n","import spacy\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import RegexpTokenizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn import preprocessing\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from sklearn.pipeline import Pipeline\n","from itertools import compress\n","from sklearn.preprocessing import StandardScaler\n","%matplotlib notebook\n"],"id":"aac51ce6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f3abb1b"},"source":["# Enter correct paths here\n","train_path = '/content/drive/MyDrive/TechSoc Submission/train.csv'\n","test_path = '/content/drive/MyDrive/TechSoc Submission/test.csv'\n","stopwords_path = '/content/drive/MyDrive/TechSoc Submission/stopwords.txt'\n","save_path='/content/drive/MyDrive/TechSoc Submission/Final.csv'\n","model_path='/content/drive/MyDrive/TechSoc Submission/mymodel.joblib'"],"id":"9f3abb1b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e33b910e"},"source":["stemmer = PorterStemmer()\n","tokenizer = RegexpTokenizer(r'\\w+')\n","Content_scaler = preprocessing.StandardScaler(with_mean=False)\n","Nouns_scaler = preprocessing.StandardScaler(with_mean=False)\n","Ent_scaler = preprocessing.StandardScaler(with_mean=False)\n","others_scaler = preprocessing.StandardScaler(with_mean=False)\n","Content_Vectorizer = CountVectorizer(ngram_range=(1, 2),max_df=0.8,min_df=1)\n","Nouns_Vectorizer = CountVectorizer()\n","Ent_Vectorizer = CountVectorizer()"],"id":"e33b910e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d91dc4ab"},"source":["stopwords = []\n","file = open(stopwords_path, \"r\")\n","for line in file:\n","    word=line.strip()\n","    stopwords.append(stemmer.stem(word))"],"id":"d91dc4ab","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e15bf629"},"source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"id":"e15bf629","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8d995051"},"source":["df = pd.read_csv(train_path)"],"id":"8d995051","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62345216"},"source":["def extra(df):\n","    docs = nlp.pipe(df.content,n_process=-1)\n","    df['Nouns']=0\n","    df['Ent']=0\n","    for i,t in enumerate(docs):\n","        ent = t.ents\n","        nouns = [word for word in t if word.pos_ == 'NOUN']\n","        ent =[word.text for word in ent]\n","        ent = ' '.join(ent)\n","        nouns =[word.text for word in nouns]\n","        nouns = ' '.join(nouns)\n","        df['Nouns'][i]=nouns\n","        df['Ent'][i]=ent\n","        if i%4000==0:\n","            print('#',end='')\n","    return df"],"id":"62345216","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"6e70ec88"},"source":["def PreProcesser(df):\n","    print('Preprocessing start...')\n","    df = df.drop(columns = ['title','uid'])\n","    df['TolWords']= df['content'].apply(lambda x: len(x.split(' ')))\n","    df['TolSentance']=df['content'].apply(lambda x: len(x))\n","    df['AvgWordLen']= df['content'].apply(lambda x: np.mean([len(word)  for word in x.split(' ') ]))\n","    print('@',end='')\n","    df = extra(df)        \n","    print('@',end='')\n","    df['content']=df['content'].apply(lambda x: TreebankWordDetokenizer().detokenize( stemmer.stem(word) for word in tokenizer.tokenize(x) ))\n","    df['content']= df['content'].apply(lambda x:TreebankWordDetokenizer().detokenize(list(compress(x.split(), [word not in stopwords  for word in x.split()]))))\n","    print('#',end='')\n","    df['Nouns']=df['Nouns'].apply(lambda x: TreebankWordDetokenizer().detokenize( stemmer.stem(word) for word in tokenizer.tokenize(x) ))\n","    df['Ent']= df['Ent'].apply(lambda x:TreebankWordDetokenizer().detokenize(list(compress(x.split(), [word not in stopwords  for word in x.split()]))))\n","    df['Ent']=df['Ent'].apply(lambda x: TreebankWordDetokenizer().detokenize( stemmer.stem(word) for word in tokenizer.tokenize(x) ))\n","    print('#',end='')\n","    df['TolSentanceFin']=df['content'].apply(lambda x: len(x))\n","    df['AvgWordLenFin']= df['content'].apply(lambda x: np.mean([len(word)  for word in x.split(' ') ]))\n","    df['TolNouns']= df['Nouns'].apply(lambda x: len(x.split(' ')))\n","    print('#',end='')\n","    df['TolEnts']= df['Ent'].apply(lambda x: len(x.split(' ')))\n","    print('Preprocessing completed')\n","    return df"],"id":"6e70ec88","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ad1983e"},"source":["def transform(df,fit=False,save=False):\n","    print('Transformation Begins')\n","    try:\n","        o = df.drop(columns=['content','Nouns','Ent','target_ind']).to_numpy()\n","    except:\n","        o = df.drop(columns=['content','Nouns','Ent']).to_numpy()\n","    else:\n","        o = df.drop(columns=['content','Nouns','Ent','target_ind']).to_numpy()\n","    if fit:\n","            Content_Vectorizer.fit(df.content)\n","            Nouns_Vectorizer.fit(df.Nouns)\n","            Ent_Vectorizer.fit(df.Ent)\n","            print('Fitted Vectorizers')\n","    c = Content_Vectorizer.transform(df['content'])\n","    n = Nouns_Vectorizer.transform(df['Nouns'])\n","    e = Ent_Vectorizer.transform(df['Ent'])\n","    print('Transformed Vectorizers')\n","    if fit:\n","            Content_scaler.fit(c)\n","            Nouns_scaler.fit(n)\n","            Ent_scaler.fit(e)\n","            others_scaler.fit(o)\n","            print('Fitted Scalers')\n","    if save:\n","        from joblib import dump,load\n","        dump(c,\"./data/c.joblib\")\n","        dump(o,\"./data/o.joblib\")\n","        dump(n,\"./data/n.joblib\")\n","        dump(e,\"./data/e.joblib\")\n","    c = Content_scaler.transform(c)\n","    n = Nouns_scaler.transform(n)\n","    e = Ent_scaler.transform(e)\n","    o = others_scaler.transform(o)\n","    print(o.shape)\n","    print('Transformed Scalers')\n","    print('Transformation Completed')\n","    return (o,c,n,e)"],"id":"7ad1983e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"dfc0f138"},"source":["def combine(o,c,n,e):\n","    print('Combiner Begins')\n","    from scipy.sparse import hstack\n","    X=hstack((o,n,e))\n","    from scipy.sparse import csr_matrix\n","    X=csr_matrix(X)\n","    return X"],"id":"dfc0f138","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"dffe89c5"},"source":["def Pipe(df,fit=False,save=False):\n","    df=PreProcesser(df)\n","    (o,c,n,e)=transform(df,fit)\n","    \n","    try:\n","        Y=df.target_ind\n","    except:\n","        Y=0\n","    else:\n","        Y=df.target_ind\n","    X = combine(o,c,n,e)\n","    if save:\n","        from joblib import dump,load\n","        \n","        dump(X,\"./data/X.joblib\")\n","        dump(Y,\"./data/Y.joblib\")\n","    return (X,Y)"],"id":"dffe89c5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c999265d"},"source":["def Process(df,fit=False,n_jobs=4):\n","    print('Using %d jobs' %n_jobs)\n","    df_split = np.array_split(df, n_jobs)\n","    from multiprocess import Pool\n","    pool = Pool(n_jobs)\n","    import functools\n","    df = pd.concat(pool.map( PreProcesser , df_split))\n","    print('Preprocessing completed')\n","    pool.close()\n","    pool.join()\n","    return df"],"id":"c999265d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"478ba023"},"source":["pd.options.mode.chained_assignment = None"],"id":"478ba023","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"a84e44ba","executionInfo":{"status":"ok","timestamp":1636718884508,"user_tz":-330,"elapsed":1021390,"user":{"displayName":"Siva Sankar S ch20b103","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09310026757114527063"}},"outputId":"83582f00-8824-4854-f561-79f139ec3018"},"source":["(X,Y)=Pipe(df,fit=True,save=False)"],"id":"a84e44ba","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing start...\n","@#########@###Preprocessing completed\n","Transformation Begins\n","Fitted Vectorizers\n","Transformed Vectorizers\n","Fitted Scalers\n","(35112, 7)\n","Transformed Scalers\n","Transformation Completed\n","Selector Begins\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"145c6137","executionInfo":{"status":"ok","timestamp":1636718884509,"user_tz":-330,"elapsed":36,"user":{"displayName":"Siva Sankar S ch20b103","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09310026757114527063"}},"outputId":"1da60bba-c206-463c-9264-acdb4031c816"},"source":["X.shape"],"id":"145c6137","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(35112, 52911)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0201cde6","executionInfo":{"status":"ok","timestamp":1636718884510,"user_tz":-330,"elapsed":26,"user":{"displayName":"Siva Sankar S ch20b103","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09310026757114527063"}},"outputId":"11568f21-06ed-41a8-8db6-1a0e692d17d7"},"source":["Y.shape"],"id":"0201cde6","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(35112,)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"63CScnZv_abi"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectFromModel\n","RF_ = RandomForestClassifier(random_state=0,max_depth = 50,verbose=1,n_jobs=4,n_estimators=100)\n","selector = SelectFromModel(estimator=RF_).fit(X[:28000],Y[:28000])\n","ind=selector.get_support()\n","X_ = X[:,ind]"],"id":"63CScnZv_abi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BErYxAEa_ior"},"source":["X_.shape"],"id":"BErYxAEa_ior","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"9ead9b4f"},"source":["from sklearn.ensemble import RandomForestClassifier\n","RF_ = RandomForestClassifier(random_state=0,max_depth = 200,verbose=2,n_jobs=-1,n_estimators=500)\n","RF_.fit(X_[:28000],Y[:28000])"],"id":"9ead9b4f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"aeaa4858"},"source":["RF_.score(X[28000:],Y[28000:])"],"id":"aeaa4858","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCG9W7rAYE43"},"source":["elapsed = time.time() - t\n","print(elapsed)"],"id":"cCG9W7rAYE43","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4AZSqRPZvp7"},"source":["from dill import dump_session\n","dump_session(model_path)"],"id":"K4AZSqRPZvp7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a444dfe5"},"source":["df_pred = pd.read_csv(test_path)"],"id":"a444dfe5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"e8942652"},"source":["(X_pred,_)=Pipe(df_pred,fit=False)"],"id":"e8942652","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"ba4529e7"},"source":["y=RF_.predict(X_pred)"],"id":"ba4529e7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f86918d0"},"source":["df_fin = pd.DataFrame(y,columns=['target_ind'])\n","df_fin.index.name='uid'\n","df_fin.to_csv(save_path,index=True)"],"id":"f86918d0","execution_count":null,"outputs":[]}]}